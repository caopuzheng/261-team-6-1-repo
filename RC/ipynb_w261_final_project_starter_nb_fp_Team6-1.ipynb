{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d49256-8961-453d-8654-fee43578144b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PLEASE CLONE THIS NOTEBOOK INTO YOUR PERSONAL FOLDER\n",
    "# DO NOT RUN CODE IN THE SHARED FOLDER\n",
    "# THERE IS A 2 POINT DEDUCTION IF YOU RUN ANYTHING IN THE SHARED FOLDER. THANKS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3feac5-60b5-4dde-8b53-4d7f0d6e55b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The following blob storage is accessible to team members only (read and write)\n",
    "# access key is valid til TTL\n",
    "# after that you will need to create a new SAS key and authenticate access again via DataBrick command line\n",
    "blob_container = \"261storagecontainer\"  # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"261storage\"  # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope = \"261_team_6_1_spring24_scope\"  # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key = \"team_6_1_key\"  # The name of the secret key created in your local computer using the Databricks CLI\n",
    "team_blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"  # points to the root of your team storage bucket\n",
    "\n",
    "\n",
    "# the 261 course blob storage is mounted here.\n",
    "mids261_mount_path = \"/mnt/mids-w261\"\n",
    "\n",
    "# SAS Token: Grant the team limited access to Azure Storage resources\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=secret_scope, key=secret_key),\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame([[1, 2, 3, \"Jane\"], [2, 2, 2, None], [12, 12, 12, \"John\"]], columns=[\"x\", \"y\", \"z\", \"a_string\"])\n",
    "df = spark.createDataFrame(pdf)  # Create a Spark dataframe from a pandas DF\n",
    "\n",
    "# The following can write the dataframe to the team's Cloud Storage\n",
    "# Navigate back to your Storage account in https://portal.azure.com, to inspect the partitions/files.\n",
    "df.write.mode(\"overwrite\").parquet(f\"{team_blob_url}/temp\")\n",
    "\n",
    "# see what's in the blob storage root folder\n",
    "display(dbutils.fs.ls(f\"{team_blob_url}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5dfdf76-196a-428c-b3c1-9d6a702c4f82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Welcome to the W261 final project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1ea3b7-c047-47db-8555-bdaee2773752",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Know your mount\n",
    "Here is the mounting for this class, your source for the original data! Remember, you only have Read access, not Write! Also, become familiar with `dbutils` the equivalent of `gcp` in DataProc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b47119-0466-4cdd-b876-35d8c4a56804",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/\"\n",
    "display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c8c905-784d-4b7c-bd23-4ff5cb612a86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "078cb1ab-1de8-4707-bbd8-6bd03fa297e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data for the Project\n",
    "\n",
    "For the project you will have 4 sources of data:\n",
    "\n",
    "1. Airlines Data: This is the raw data of flights information. You have 3 months, 6 months, 1 year, and full data from 2015 to 2019. Remember the maxima: \"Test, Test, Test\", so a lot of testing in smaller samples before scaling up! Location of the data? `dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/`, `dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_1y/`, etc. (Below the dbutils to get the folders)\n",
    "2. Weather Data: Raw data for weather information. Same as before, we are sharing 3 months, 6 months, 1 year\n",
    "3. Stations data: Extra information of the location of the different weather stations. Location `dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/`\n",
    "4. OTPW Data: This is our joined data (We joined Airlines and Weather). This is the main dataset for your project, the previous 3 are given for reference. You can attempt your own join for Extra Credit. Location `dbfs:/mnt/mids-w261/OTPW_60M/` and more, several samples are given!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa2b72c9-932d-420c-8e87-acba56e86382",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airline Data\n",
    "df_flights = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_3m/\")\n",
    "display(df_flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a231d5f-5415-4a29-a247-aa911a202e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weather data\n",
    "df_weather = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_3m/\")\n",
    "display(df_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18ebbfe-51a1-40e6-85a6-62a4721ba387",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stations data\n",
    "df_stations = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/\")\n",
    "display(df_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889b5d58-a586-4aa1-95d2-44591814e18b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OTPW\n",
    "df_otpw = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"dbfs:/mnt/mids-w261/OTPW_3M_2015.csv\")\n",
    "display(df_otpw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88dd020-5006-438c-9557-2d8fbab054a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Example of EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e15e3153-81f0-415b-8a24-3a5b00384d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_weather = spark.read.parquet(\n",
    "    f\"{data_BASE_DIR}datasets_final_project_2022/parquet_weather_data_3m/\"\n",
    ")\n",
    "\n",
    "# Grouping and aggregation for df_stations\n",
    "grouped_stations = (\n",
    "    df_stations.groupBy(\"neighbor_id\")\n",
    "    .agg(\n",
    "        F.avg(\"distance_to_neighbor\").alias(\"avg_distance_to_neighbor\"),\n",
    "    )\n",
    "    .orderBy(\"avg_distance_to_neighbor\")\n",
    ")\n",
    "\n",
    "display(grouped_stations)\n",
    "\n",
    "# Grouping and aggregation for df_flights\n",
    "grouped_flights = df_flights.groupBy(\"OP_UNIQUE_CARRIER\").agg(\n",
    "    F.avg(\"DEP_DELAY\").alias(\"Avg_DEP_DELAY\"),\n",
    "    F.avg(\"ARR_DELAY\").alias(\"Avg_ARR_DELAY\"),\n",
    "    F.avg(\"DISTANCE\").alias(\"Avg_DISTANCE\"),\n",
    ")\n",
    "\n",
    "display(grouped_flights)\n",
    "\n",
    "# Convert columns to appropriate data types\n",
    "df_weather = df_weather.withColumn(\n",
    "    \"HourlyPrecipitationDouble\", F.col(\"HourlyPrecipitation\").cast(\"double\")\n",
    ")\n",
    "df_weather = df_weather.withColumn(\n",
    "    \"HourlyVisibilityDouble\", F.col(\"HourlyVisibility\").cast(\"double\")\n",
    ")\n",
    "df_weather = df_weather.withColumn(\n",
    "    \"HourlyWindSpeedDouble\", F.col(\"HourlyWindSpeed\").cast(\"double\")\n",
    ").filter(col(\"HourlyWindSpeedDouble\") < 2000)\n",
    "\n",
    "# Overlayed boxplots for df_weather\n",
    "weather_cols = [\n",
    "    \"HourlyPrecipitationDouble\",\n",
    "    \"HourlyVisibilityDouble\",\n",
    "    \"HourlyWindSpeedDouble\",\n",
    "]\n",
    "weather_data = df_weather.select(*weather_cols).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "weather_data.boxplot(column=weather_cols)\n",
    "plt.title(\"Boxplots of Weather Variables\")\n",
    "plt.xlabel(\"Weather Variables\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05c0fff-7ddc-44c7-985b-00311dade7c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pipeline Steps For Classification Problem\n",
    "\n",
    "These are the \"normal\" steps for a Classification Pipeline! Of course, you can try more!\n",
    "\n",
    "## 1. Data cleaning and preprocessing\n",
    "\n",
    "* Remove outliers or missing values\n",
    "* Encode categorical features\n",
    "* Scale numerical features\n",
    "\n",
    "## 2. Feature selection\n",
    "\n",
    "* Select the most important features for the model\n",
    "* Use univariate feature selection, recursive feature elimination, or random forest feature importance\n",
    "\n",
    "## 3. Model training\n",
    "\n",
    "* Train a machine learning model to predict delays more than 15 minutes\n",
    "* Use logistic regression, decision trees, random forests, or support vector machines\n",
    "\n",
    "## 4. Model evaluation\n",
    "\n",
    "* Evaluate the performance of the trained model on a holdout dataset\n",
    "* Use accuracy, precision, recall, or F1 score\n",
    "\n",
    "## 5. Model deployment\n",
    "\n",
    "* Deploy the trained model to a production environment\n",
    "* Deploy the model as a web service or as a mobile app\n",
    "\n",
    "## Tools\n",
    "\n",
    "* Spark's MLlib and SparkML libraries\n",
    "* These libraries have parallelized methods for data cleaning and preprocessing, feature selection, model training, model evaluation, and model deployment which we will utilize for this classification problem.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ipynb_w261_final_project_starter_nb_fp_Team6-1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
