{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5872e2cd-d7d8-4237-993d-f86c51871bdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PLEASE CLONE THIS NOTEBOOK INTO YOUR PERSONAL FOLDER and DO NOT RUN CODE IN THE SHARED FOLDER\n",
    "\n",
    "# THERE IS A 2 POINT DEDUCTION IF YOU RUN ANYTHING IN THE SHARED FOLDER. THANKS!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5e82a06-55e2-4189-9a82-2eac31c5feb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setting up your Team's Cloud Storage on Azure\n",
    "\n",
    "Each team will need to create a blob storage area on Azure. This will be created by one team member known as the Storage Team Lead (possibly with an another team member as an observer). Once the blob storage is created the Storage Lead Person will give access to all other team members via shared secrets (read on to learn more). Then all team members (and only team members) will be have access to the team storage bucket. Read on to learn how to do this.\n",
    "\n",
    "## Create storage bucket (performed by Storage Lead on your project team)\n",
    "\n",
    "The Storage Lead Person in your team will need to perform the following steps to create storage bucket for your team:\n",
    "\n",
    "- Download Databricks CLI to your laptop\n",
    "- Create Azure Blob Storage\n",
    "- Generate access credentials via **SAS Token**\n",
    "- Share Blob storage access Credentials via Databricks Secrets\n",
    "- Set up a code cell that can be pasted into any notebook that is used by the project team thereby giving them access to the team's blob storage\n",
    "\n",
    "## Read/Write to cloud storage from DataBricks cluster (can be tested by any team member)\n",
    "\n",
    "Now that a blob store (aka container has been created), any member of the team can read and write from the team's blob storage (aka container in Azure jargon).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98d7811d-bc1c-4424-89cd-e8ddd3e4ada2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create storage bucket (performed by one person on your team)\n",
    "\n",
    "## Download Databricks CLI to your laptop\n",
    "\n",
    "**Note:** All Databricks CLI commands should be run on your `local computer`, not on the cluster.\n",
    "\n",
    "- On your LOCAL LAPTOP, please install the Databricks CLI by running this command:\n",
    "  - `python3 -m pip install databricks-cli`\n",
    "- To access information through Databricks CLI, you have to authenticate. For authenticating and accessing the Databricks REST APIs, you have to use a personal access token.\n",
    "\n",
    "  - To generate the access token, click on the user profile icon in the top right corner of the Databricks Workspace and select user settings.\n",
    "    - Go to the extreme top right corner of this notebook UI and make sure the NAVIGATATION BAR (which is a separate bar about the notebook menu bar) and click on the dropdown menu associated with you email **...@berkeley.edu**, then click on **User Settings**,\n",
    "  - Enter the name of the comment and lifetime (total validity days of the token).\n",
    "  - Click on generate.\n",
    "  - Now, the Personal Access is generated; copy the generated token.\n",
    "    - NOTE: once you generate a token you will only have one chance to copy the token to a safe place.\n",
    "\n",
    "- In the command prompt, type `databricks configure –token` and press enter.\n",
    "\n",
    "  - When prompted to enter the Databricks Host URL, provide your Databricks Host Link which is `https://adb-4248444930383559.19.azuredatabricks.net`.\n",
    "  - Then, you will be asked to enter the token. Enter your generated TOKEN and authenticate.\n",
    "\n",
    "- Now, you are successfully authenticated and all set for creating Secret Scopes and Secrets using CLI (see below). Secret Scopes and Secrets help to avoid sharing passwords and access keys in your notebooks.\n",
    "- NOTE: you can also see this TOKEN via the command line by typing the following command on your Terminal window.\n",
    "  `Jamess-MacBook-Pro-10:~ jshanahan$      cat ~/.databrickscfg`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d636aa9a-0443-4106-be1e-0a6b18c640b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Azure Blob Storage and generate access priviledges\n",
    "\n",
    "**Special Note:** Creating a Storage account, only needs to be performed by **one** member of the team. This person then creates a blob storage area (known as a container) and shares access credentials with the rest of the team via a Secrets ACL. Please be responsible.\n",
    "\n",
    "### Create Storage Account\n",
    "\n",
    "1. Navigate to https://portal.azure.com\n",
    "2. Login using Calnet credentials *myuser@berkeley.edu*\n",
    "3. Click on the top right corner on the User Icon.\n",
    "4. Click on Switch directory. Make sure you switch to **UC Berkeley berkeley.onmicrosoft.com**, this would be your personal space.\n",
    "5. Click on the Hamburger Menu Icon on the top left corner, navigate to **Storage accounts**.\n",
    "6. Choose the option **Azure for Students** to take advantage of $100 in credits. Provide you _berkeley.edu_ email and follow the prompts.\n",
    "7. Once the subscription is in place, navigate back to Storage accounts, refresh if needed. Hit the button **+ Create** in the top menu.\n",
    "\n",
    "- Choose **Azure for Students** as Subscription (think billing account).\n",
    "- Create a new Resource group. Name is irrelevant here.\n",
    "- Choose a **Storage account name**, you will need this in the _Init Script_ below. (e.g., jshanahan). This a master directory within which we have blob storages, aka containers on Azure.\n",
    "- Go with the defaults for the rest of the form.\n",
    "- Hit the **Review + create** button.\n",
    "\n",
    "8. Once the **Storage account** is shown in your list:\n",
    "\n",
    "- Click on it. This will open a sub-window.\n",
    "- Under _Data Storage_, click on **container**.\n",
    "- Hit the **+ Container** in the top menu.\n",
    "- Choose a name for your container; to access this container you will need to generate a SAS token in the _Init Script_ below.\n",
    "\n",
    "**Note:** Create your Blob Storage in the US West 2 Region.\n",
    "\n",
    "### Obtain Credentials via **SAS Token** or via **Access Key**\n",
    "\n",
    "First, you need to choose between using a SAS token (or via Access Key) to enable access to you blob storage. Bottom line, SAS tokens would be recommended since it's a token in which you have control on permissions and TTL (Time to Live). On the other hand, an Access Key, would grant full access to the Storage Account and will generate SAS tokens in the backend when these expire.\n",
    "\n",
    "To obtain a **SAS Token** which is the recommended way to offer access to your team mates.\n",
    "\n",
    "SAS Token (Shared Access Signature token) that offers access for a restricted time period which we recommend:\n",
    "\n",
    "1. Navigate to the containers list.\n",
    "2. At the far right, click on the `...` for the container you just created.\n",
    "3. Check the boxes of the permissions you want.\n",
    "4. Select an expiration you are comfortable with.\n",
    "5. Hit the **Generate SAS token and URL** button.\n",
    "6. Scroll down and copy only the **Blob SAS token**.\n",
    "\n",
    "Please try to avoid using **Access Key**:\n",
    "\n",
    "To obtain the **Access Key** (unrestricted access as long as you have the token):\n",
    "\n",
    "1. Navigate back to \\*Storage accounts\\*\\*.\n",
    "2. Click on the recently created account name.\n",
    "3. In the sub-window, under _Security + networking_, click on **Access Keys**.\n",
    "4. Hit the **Show keys** button.\n",
    "5. Copy the **Key**, you don't need the Connection string. It's irrelevant if you choose _key1_ or _key2_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d72f1cdf-121a-4c9d-b270-0c431e0e84f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Share Blob storage access credentials securely via Databricks Secret\n",
    "\n",
    "Now, you are successfully authenticated next we will create Secret Scopes and Secrets using the CLI to avoid sharing passwords and access keys in your notebooks.\n",
    "\n",
    "#### Some background on scopes and secrets [you can skip this subsection if you are low on time]\n",
    "\n",
    "Since security is the primary concern when working with Cloud services, instead of storing passwords or access keys in Notebook or Code in plaintext, Databricks/Azure provides two types of Secret Scopes to store and retrieve all the secrets when and where they are needed. In Databricks, every Workspace has Secret Scopes within which one or more Secrets are present to access third-party data, integrate with applications, or fetch information. Users can also create multiple Secret Scopes within the workspace according to the demand of the application.\n",
    "\n",
    "The two types of Databricks Secret Scopes are:\n",
    "\n",
    "- 1. Azure Key Vault-Backed Scope [not applicable here; see Azure documentation for more details]\n",
    "\n",
    "- 2. Databricks-Backed Scope\n",
    "     In this method, the Secret Scopes are managed with an internally encrypted database owned by the Databricks platform. Users can create a Databricks-backed Secret Scope using the Databricks CLI version 0.7.1 and above.\n",
    "\n",
    "#### Permission Levels of Secret Scopes\n",
    "\n",
    "There are three levels of permissions that you can assign while creating each Secret Ccope. They are:\n",
    "\n",
    "- Manage: This permission is used to manage everything about the Secret Scopes and ACLS (Access Control List). By using ACLs, users can configure fine-grained permissions to different people and groups for accessing different Scopes and Secrets.\n",
    "- Write: This allows you to read, write, and manage the keys of the particular Secret Scope.\n",
    "- Read: This allows you to read the secret scope and list all the secrets available inside it.\n",
    "\n",
    "### Creating Secret Scopes and Secrets using Databricks CLI (to avoid sharing passwords and access keys in your notebooks)\n",
    "\n",
    "**Special Note:** Only the member that created the Storage account should perform this step.\n",
    "\n",
    "1. On your laptop via the CLI, create a **SCOPE**:\n",
    "\n",
    "- `databricks secrets create-scope --scope <choose-any-name>`\n",
    "\n",
    "2. Next create Secrets inside the Secret Scope using Databricks CLI\n",
    "   You can enter the following command to create a Secret inside the Scope: On your laptop via the CLI, load the key/token:\n",
    "\n",
    "- `databricks secrets put --scope <name-from-above> --key <choose-any-name> --string-value '<paste-key-SAS-token-here>'`\n",
    "\n",
    "NOTE --principal should be CLUSTER Name\n",
    "\n",
    "3.  On your laptop via the CLI, add a `principal` to the Secret Scope ACL to share token with your teammates. This is done at the team cluster level, so you will need the name of your Databricks cluster. **Careful:** make sure you type the right cluster name.\n",
    "\n",
    "- `databricks secrets put-acl --scope <name-from-above> --principal \"Data Bricks CLUSTER-Name\"  --permission READ`\n",
    "\n",
    "Putting all three steps together, it might look like this for a sample project team who is running on a Databricks cluster called `team 1-1`:\n",
    "\n",
    "```bash\n",
    "databricks secrets create-scope 261_team_6_1_spring24_scope   #made a scope of jgs_instructors;\n",
    "databricks secrets put-secret 261_team_6_1_spring24_scope team_6_1_key \\\n",
    "        --string-value 'sp=racwdli&st=2024-03-11T00:49:59......'\n",
    "databricks secrets put-acl 261_team_6_1_spring24_scope \"team_6-1\" READ  #assume my DataBricks cluster name is team_6-1\n",
    "\n",
    "```\n",
    "\n",
    "**Note:** This has been tested only on Mac/Linux. It might be different in Windows.\n",
    "\n",
    "- For Windows: to load the key/ SAS token, replace the single quote `''` with double quote `\"\"`.\n",
    "  `databricks secrets put --scope <name-from-above> --key <choose-any-name> --string-value \"<paste-key-SAS-token-here>\"`\n",
    "\n",
    "Then each team members could run the following, there by saving a small Spark dataframe to the team's blob storage. Then any team member can see the saved data on the team blob storage `test` via https://portal.azure.com:\n",
    "\n",
    "```python\n",
    "secret_scope = \"261_team_6_1_spring24_scope\"\n",
    "secret_key   = \"team_1_1_key\"\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")\n",
    "blob_container  = \"my_container_name\"       # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"my_storage_account_name\" # The name of your Storage account created in https://portal.azure.com\n",
    "team_blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\n",
    "\n",
    "pdf = pd.DataFrame([[1, 2, 3, \"Jane\"], [2, 2,2, None], [12, 12,12, \"John\"]], columns=[\"x\", \"y\", \"z\", \"a_string\"])\n",
    "df = spark.createDataFrame(pdf) # Create a Spark dataframe from a pandas DF\n",
    "\n",
    "# The following can write the dataframe to the team's Cloud Storage\n",
    "# Navigate back to your Storage account in https://portal.azure.com, to inspect the partitions/files.\n",
    "df.write.parquet(f\"{team_blob_url}/test\")\n",
    "\n",
    "# see what's in the parquet folder\n",
    "display(dbutils.fs.ls(f\"{team_blob_url}/test\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cddad987-a7fb-49a2-a95d-6980de874624",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Read/write to blob storage (for all team members)\n",
    "\n",
    "## Init Script\n",
    "\n",
    "The Storage Team Lead will need to adapt the following cell so that team members can read/write to the team's blob storage.\n",
    "\n",
    "Please replace these variable values with your blob storage details and access credential information:\n",
    "\n",
    "```python\n",
    "blob_container  = “my_container_name”        # The name of your container created in https://portal.azure.com\n",
    "storage_account = “my_storage_account_name”  # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope    = “team_1_1_scope”           # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key      = “team_1_1_key”             # The name of the secret key created in your local computer using the Databricks CLI\n",
    "```\n",
    "\n",
    "This cell can then be copied to any team notebook that needs access to the team cloud storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4289309d-e324-4b8b-9577-9fc0a4e56455",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Place this cell in any team notebook that needs access to the team cloud storage.\n",
    "\n",
    "\n",
    "# The following blob storage is accessible to team members only (read and write)\n",
    "# access key is valid til TTL\n",
    "# after that you will need to create a new SAS key and authenticate access again via DataBrick command line\n",
    "blob_container = \"261storagecontainer\"  # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"261storage\"  # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope = \"261_team_6_1_spring24_scope\"  # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key = \"team_6_1_key\"  # The name of the secret key created in your local computer using the Databricks CLI\n",
    "team_blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"  # points to the root of your team storage bucket\n",
    "\n",
    "\n",
    "# the 261 course blob storage is mounted here.\n",
    "mids261_mount_path = \"/mnt/mids-w261\"\n",
    "\n",
    "# SAS Token: Grant the team limited access to Azure Storage resources\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=secret_scope, key=secret_key),\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame([[1, 2, 3, \"Jane\"], [2, 2, 2, None], [12, 12, 12, \"John\"]], columns=[\"x\", \"y\", \"z\", \"a_string\"])\n",
    "df = spark.createDataFrame(pdf)  # Create a Spark dataframe from a pandas DF\n",
    "\n",
    "# The following can write the dataframe to the team's Cloud Storage\n",
    "# Navigate back to your Storage account in https://portal.azure.com, to inspect the partitions/files.\n",
    "df.write.mode(\"overwrite\").parquet(f\"{team_blob_url}/TP\")\n",
    "\n",
    "# see what's in the blob storage root folder\n",
    "display(dbutils.fs.ls(f\"{team_blob_url}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8463aa5d-8576-48af-a211-e79428078763",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read and write data!\n",
    "\n",
    "A _Read Only_ mount has been made available to all course clusters in this Databricks Platform. It contains data you will use for **HW5** and **Final Project**. Feel free to explore the files by running the cell below. Read them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3806471-e1ea-4861-8e69-a11a59209ed9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{mids261_mount_path}/datasets_final_project_2022\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f73e923-5cc7-4c73-907d-dc5d154536c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count, col, split, trim, lit, avg, sum\n",
    "\n",
    "df_airlines = spark.read.parquet(f\"{mids261_mount_path}/datasets_final_project/parquet_airlines_data_3m/\")  # Load the Jan 1st, 2015 for Weather\n",
    "df_weather = spark.read.parquet(f\"{mids261_mount_path}/datasets_final_project/weather_data/*\").filter(col(\"DATE\") < \"2015-01-02T00:00:00000\").cache()\n",
    "display(df_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b803be2c-539d-4899-8c53-a22cefbbd3d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This command will write to your Cloud Storage if right permissions are in place.\n",
    "# Navigate back to your Storage account in https://portal.azure.com, to inspect the files.\n",
    "df_weather.write.mode(\"overwrite\").parquet(f\"{team_blob_url}/TP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f691cb8-ff93-4b1e-b95a-7d23891ff436",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# see what's in the parquet folder\n",
    "display(dbutils.fs.ls(f\"{team_blob_url}/TP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90a1d28-8782-4b4a-9626-fada3f53f77c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load it the previous DF as a new DF\n",
    "df_weather_new = spark.read.parquet(f\"{team_blob_url}/TP\")\n",
    "display(df_weather_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10054be2-a13d-48ba-8e5b-2c2791170439",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Your new df_weather has {df_weather_new.count():,} rows.\")\n",
    "print(f'Max date: {df_weather_new.select([max(\"DATE\")]).collect()[0][\"max(DATE)\"].strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0cac848-52ca-4543-9577-68e32394c043",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{mids261_mount_path}/HW5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91857275-f911-46f8-9a62-282175194e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# [DEPRECATED]\n",
    "\n",
    "### Using RDD API\n",
    "\n",
    "When reading/writing using the RDD API, configuration cannot happen at runtime but at cluster creation.\n",
    "If you need the following information to be added in your Cluster as Spark Configuration when running RDD API, ping TA team. You normally do not need this set up for the final project.\n",
    "\n",
    "- Storage Account name\n",
    "- Container name\n",
    "- Secret Scope name\n",
    "- Secret Key name\n",
    "\n",
    "**Important:** Do not share the actual SAS token.\n",
    "\n",
    "After this is added as Spark Configuration, try the scripts provided below to test the Hadoop plug-in to connect to your Azure Blob Storage.\n",
    "\n",
    "```\n",
    "spark.hadoop.fs.azure.sas.<container_name>.<storage_account>.blob.core.windows.net {{secrets/<scope>/<key>}}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6441715-c94a-4972-944c-fa7a56dbb244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/mnt/mids-w261/HW5/test_graph.txt\")\n",
    "\n",
    "\n",
    "parsed_rdd = rdd.map(lambda line: tuple(line.split(\"\\t\")))\n",
    "parsed_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c237cf94-fc09-4201-8050-901aa1368e59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_rdd.saveAsTextFile(f\"{blob_url}/graph_test\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_and_mount_team_cloud_storage_Team6-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
